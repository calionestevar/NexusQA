# ================================================================================
# WORKFLOW DISABLED FOR PORTFOLIO VERSION
# 
# This workflow requires a self-hosted GitHub Actions runner with Unreal Engine
# 5.6 installed. In a production environment, this would:
# - Run the full Nexus test suite
# - Execute performance benchmarks
# - Generate comprehensive test reports
# - Upload artifacts for review
#
# To enable: Rename to aslan-vigil.yml and configure a self-hosted runner with
# UE5 at the path specified in UE_ENGINE_PATH secret.
# ================================================================================

name: Aslan's Vigil - Main CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  nexus-windows-tests:
    name: Run Nexus harness (Windows)
    runs-on: windows-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Prepare Saved dir
        run: |
          if (!(Test-Path Saved)) { New-Item -ItemType Directory -Path Saved }
        shell: pwsh

      - name: Run Engage.ps1 (canonical harness)
        # NOTE: This step expects you to provide a runner with Unreal installed or to set
        # the `UE_ENGINE_PATH` repository secret to the engine root path.
        run: |
          Write-Host 'Starting Engage.ps1. If Unreal is not available this step will likely fail.'
          $exit = 0
          try {
            & pwsh -NoProfile -ExecutionPolicy Bypass -File .\Scripts\Engage.ps1
            $exit = $LASTEXITCODE
          } catch {
            Write-Host "Engage.ps1 failed to start: $_"
            $exit = 2
          }
          exit $exit
        shell: pwsh

      - name: Upload Saved artifacts (if any)
        uses: actions/upload-artifact@v4
        with:
          name: nexus-saved-artifacts
          path: Saved/**

      - name: Upload NexusReports (HTML + JUnit XML)
        uses: actions/upload-artifact@v4
        with:
          name: nexus-reports
          path: Saved/NexusReports/**
      
      - name: Upload Trace Exports (Distributed Tracing)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: trace-exports
          path: |
            Saved/NexusReports/traces.jsonl
            Saved/NexusReports/*_trace.json
      
      - name: Generate Test Summary
        if: always()
        run: |
          $xmlPath = Join-Path $PWD 'Saved\NexusReports\nexus-results.xml'
          if (Test-Path $xmlPath) {
            [xml]$xml = Get-Content $xmlPath
            $total = [int]$xml.testsuite.tests
            $failures = [int]$xml.testsuite.failures
            $passed = $total - $failures
            $passRate = [math]::Round(($passed / $total) * 100, 1)
            
            Write-Host "=========================================="
            Write-Host "NEXUS TEST SUMMARY"
            Write-Host "=========================================="
            Write-Host "Total Tests:   $total"
            Write-Host "Passed:        $passed"
            Write-Host "Failed:        $failures"
            Write-Host "Pass Rate:     $passRate%"
            Write-Host "=========================================="
            
            # Write to GitHub step summary
            Add-Content $env:GITHUB_STEP_SUMMARY "## Nexus Test Results`n"
            Add-Content $env:GITHUB_STEP_SUMMARY "- **Total Tests:** $total`n"
            Add-Content $env:GITHUB_STEP_SUMMARY "- **Passed:** âœ… $passed`n"
            Add-Content $env:GITHUB_STEP_SUMMARY "- **Failed:** âŒ $failures`n"
            Add-Content $env:GITHUB_STEP_SUMMARY "- **Pass Rate:** $passRate%`n"
            
            if ($failures -gt 0) {
              Add-Content $env:GITHUB_STEP_SUMMARY "`n### Failed Tests`n"
              $xml.testsuite.testcase | Where-Object { $_.failure } | ForEach-Object {
                $name = "$($_.classname).$($_.name)"
                $msg = $_.failure.message
                Add-Content $env:GITHUB_STEP_SUMMARY "- âŒ ``$name``: $msg`n"
              }
            }
          } else {
            Write-Host "No JUnit XML found; skipping summary"
          }
        shell: pwsh

      - name: Evaluate JUnit results (if present)
        run: |
          $xmlPath = Join-Path $PWD 'Saved\NexusReports\nexus-results.xml'
          if (Test-Path $xmlPath) {
            $content = Get-Content $xmlPath -Raw
            if ($content -match 'failures=\"(\d+)\"') {
              $failures = [int]$matches[1]
              Write-Host "Detected $failures test failures in JUnit report"
              if ($failures -gt 0) { exit 1 }
            } else {
              Write-Host "No failures attribute found in JUnit report; continuing"
            }
          } else {
            Write-Host "No JUnit report found; skipping evaluation"
          }
        shell: pwsh

      - name: Fail if Engage exited nonzero
        if: failure()
        run: exit 1
  
  api-tests:
    name: API & Network Tests (OnlineOnly)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Test Public API Endpoints
        run: |
          echo "=========================================="
          echo "PALANTÃR API TESTING DEMONSTRATION"
          echo "=========================================="
          echo ""
          echo "Testing public API endpoints to demonstrate"
          echo "PalantÃ­rRequest REST/GraphQL capabilities..."
          echo ""
          
          # Test JSONPlaceholder API (public test API)
          echo "â†’ Testing JSONPlaceholder API..."
          response=$(curl -s -w "\n%{http_code}" -H "X-Trace-ID: ci-test-$(uuidgen)" \
            https://jsonplaceholder.typicode.com/users/1)
          status=$(echo "$response" | tail -n1)
          
          if [ "$status" -eq 200 ]; then
            echo "  âœ… GET /users/1 returned 200 OK"
          else
            echo "  âŒ GET /users/1 returned $status"
            exit 1
          fi
          
          # Test POST endpoint
          echo "â†’ Testing POST endpoint..."
          post_response=$(curl -s -w "\n%{http_code}" -X POST \
            -H "Content-Type: application/json" \
            -H "X-Trace-ID: ci-test-$(uuidgen)" \
            -d '{"title":"Test Post","body":"Test Body","userId":1}' \
            https://jsonplaceholder.typicode.com/posts)
          post_status=$(echo "$post_response" | tail -n1)
          
          if [ "$post_status" -eq 201 ]; then
            echo "  âœ… POST /posts returned 201 Created"
          else
            echo "  âŒ POST /posts returned $post_status"
            exit 1
          fi
          
          # Test GraphQL endpoint (SpaceX API)
          echo "â†’ Testing GraphQL endpoint..."
          gql_response=$(curl -s -w "\n%{http_code}" -X POST \
            -H "Content-Type: application/json" \
            -H "X-Trace-ID: ci-test-$(uuidgen)" \
            -d '{"query":"{ company { name ceo } }"}' \
            https://api.spacex.land/graphql/)
          gql_status=$(echo "$gql_response" | tail -n1)
          
          if [ "$gql_status" -eq 200 ]; then
            echo "  âœ… GraphQL query returned 200 OK"
          else
            echo "  âŒ GraphQL query returned $gql_status"
            exit 1
          fi
          
          echo ""
          echo "=========================================="
          echo "âœ… ALL API TESTS PASSED"
          echo "=========================================="
          echo ""
          echo "In Unreal Engine, these same endpoints are tested using:"
          echo "  FPalantirRequest::Get(...).ExpectStatus(200).ExecuteBlocking()"
          echo ""
          echo "With automatic trace ID injection for distributed tracing."
      
      - name: Generate API Test Report
        if: always()
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          ## PalantÃ­r API Testing
          
          Demonstrated REST & GraphQL validation capabilities:
          
          ### âœ… Endpoints Tested
          - **JSONPlaceholder** (REST): GET /users/1, POST /posts
          - **SpaceX API** (GraphQL): Company query
          
          ### ðŸ” Trace Correlation
          All requests include `X-Trace-ID` header for distributed tracing:
          ```
          X-Trace-ID: ci-test-<UUID>
          User-Agent: GitHub-Actions-CI
          ```
          
          ### ðŸ“š Framework Capabilities
          In Unreal Engine tests, PalantÃ­rRequest provides:
          - Fluent API: `.WithHeader().WithRetry().ExpectStatus().ExecuteBlocking()`
          - JSONPath validation: `.ExpectJSON("user.name", "John")`
          - Automatic trace propagation to Sentry, PlayFab, GameAnalytics
          - Retry logic with exponential backoff
          - GraphQL query support with variables
          
          See `docs/API_TESTING.md` for complete documentation.
          EOF

  valkyrie-dryrun:
    name: Valkyrie (dry-run validation)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml

      - name: Validate cair-paravel.yml syntax
        run: |
          python - <<'PY'
import yaml, sys
path = '.github/workflows/cair-paravel.yml'
try:
    with open(path, 'r') as f:
        data = yaml.safe_load(f)
    print('cair-paravel.yml loaded successfully')
    # Print high-level summary
    jobs = data.get('jobs', {})
    print(f"Found jobs: {', '.join(jobs.keys())}")
    triggers = data.get('on', {})
    print('Triggers:')
    for k,v in triggers.items():
        print('  ', k)
except Exception as e:
    print('Failed to parse cair-paravel.yml:', e)
    sys.exit(2)
PY

      - name: Valkyrie dry-run simulation (no PRs created)
        run: |
          echo "Simulating Valkyrie actions..."
          echo "(This dry-run will not create PRs. To activate, run valkyrie workflow directly.)"
          # For demonstration, list package managers detected
          if [ -f vcpkg.json ]; then echo "vcpkg present"; fi
          if ls conanfile* 1> /dev/null 2>&1; then echo "conanfile present"; fi
          if ls *.csproj 1> /dev/null 2>&1; then echo "csproj present"; fi

# Helpful notes for repository maintainers:
# - Running Unreal Engine in GitHub Actions requires either self-hosted runners with UE installed or
#   careful configuration of caching and licensing. This workflow is intentionally a skeleton that
#   demonstrates how recruiters/maintainers can wire the `Engage.ps1` harness into CI. Customize
#  the steps to build the Editor, set the `UE_ENGINE_PATH` environment variable, and handle
#   licensing as needed.
